{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "Generative Adversarial Networks or GANs are recent unsupervised techniques (introduced in 2014 by Ian Goodfellow) to train neural networks to generate plausible data using a zero sum game and then outputs the entire output value - such as an image in one shot.\n",
    "\n",
    "In this lecture, I'll be building a GAN trained on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How GANS Work:\n",
    "\n",
    "GANs make use of two neural networks instead of one. The first neural network - called the generator - uses a differentiable function as a genenrator network. The generator takes some random noise as input and transforms it to have a recognizable structure such as a realistic image\n",
    "\n",
    "<img src=\"images/2.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### How do you train the Generator to do this?\n",
    "\n",
    "The generator is trained differently from a normal supervised model. It's just shown a bunch of images and told to make more images that come from the same probability distribution. This is where a second neural network, called the discriminator, comes in which learns to guide the generator.\n",
    "\n",
    "The discriminator is just a regular neural network classifier. During the training process the discriminator is shown real images from the training data half the time and fake images from the generator the other half. The discrimanor is trained to output the probability that the input is real so it tries to assign a probability near 1 to real images and a probability near 0 to fake images\n",
    "\n",
    "<img src=\"images/3.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ian Goodfellow's orignal paper talks about the generator and discrinimator playing the following two player min max game:\n",
    "\n",
    "<img src=\"images/6.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "Two optimization algorithms are run simultaneously each minimizing one of the network’s cost with respect to its parameters. The equilibrium occurs at a maximum for the discriminator and a minimum for the generator.\n",
    "\n",
    "### How do we achieve this equilibrium?\n",
    "\n",
    "The generator takes random values Z and maps them to output values X. Wherever the generator maps more values of Z, the probability distribution over X represented by the model becomes denser . The discriminator outputs high values wherever the density of real data is greater than the density of generated data. The generator changes the samples it produces to move uphill along the function learned by the discriminator. In other words, the generator moves its samples into areas where the model distribution is not yet dense enough.\n",
    "\n",
    "Eventually, the generator’s distribution matches the real distribution and the discriminator has to output a probability of one half everywhere because every point is equally likely to be generated by the real dataset as to be generated by the model because the two densities are now equal as can be seen from the image above (Again, taken from Ian Goodfellow’s original paper on GANs)\n",
    "\n",
    "<img src=\"images/4.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## A GAN Model To Generate Realistic Face Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from matplotlib import pyplot\n",
    "from image_loader import Imageset\n",
    "import imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Defining the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(image_width, image_height, image_channels, z_dim):\n",
    "    \n",
    "    real = tf.placeholder(tf.float32, (None, image_width, image_height, image_channels), name='real')\n",
    "    z = tf.placeholder(tf.float32, shape=(None, z_dim), name='z')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    return real, z, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discriminator(images, reuse=False):\n",
    "    \n",
    "    alpha = 0.2\n",
    "    \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        \n",
    "        x1 = tf.layers.conv2d(images, 128, 5, strides=2, padding='same')\n",
    "        relu1 = tf.maximum(alpha * x1, x1)\n",
    "        \n",
    "        \n",
    "        x2 = tf.layers.conv2d(relu1, 256, 5, strides=2, padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(x2, training=True)\n",
    "        relu2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        \n",
    "        x3 = tf.layers.conv2d(relu2, 512, 5, strides=2, padding='same')\n",
    "        bn3 = tf.layers.batch_normalization(x3, training=True)\n",
    "        relu3 = tf.maximum(alpha * bn3, bn3)\n",
    "        \n",
    "\n",
    "        flat = tf.reshape(relu3, (-1, 4*4*512))\n",
    "        logits = tf.layers.dense(flat, 1)\n",
    "        out = tf.sigmoid(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generator(z, out_channel_dim, is_train=True):\n",
    "    \n",
    "    alpha = 0.2\n",
    "    \n",
    "    with tf.variable_scope('generator', reuse = not is_train):\n",
    "        \n",
    "        x1 = tf.layers.dense(z, 7*7*512)\n",
    "        # Reshape it to start the convolutional stack\n",
    "        x1 = tf.reshape(x1, (-1, 7, 7, 512))\n",
    "        x1 = tf.layers.batch_normalization(x1, training=is_train)\n",
    "        x1 = tf.maximum(alpha * x1, x1)\n",
    "        \n",
    "        x2 = tf.layers.conv2d_transpose(x1, 256, 5, strides=2, padding='same')\n",
    "        x2 = tf.layers.batch_normalization(x2, training=is_train)\n",
    "        x2 = tf.maximum(alpha * x2, x2)\n",
    "        \n",
    "        x3 = tf.layers.conv2d_transpose(x2, 128, 5, strides=2, padding='same')\n",
    "        x3 = tf.layers.batch_normalization(x3, training=is_train)\n",
    "        x3 = tf.maximum(alpha * x3, x3)\n",
    "        \n",
    "        logits = tf.layers.conv2d_transpose(x3, out_channel_dim, 5, strides=1, \n",
    "                                            padding='same')\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Defining Losses and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, out_channel_dim):\n",
    "    \n",
    "    generator_model = generator(input_z, out_channel_dim)\n",
    "    discriminator_model_real, discriminator_logits_real = discriminator(input_real)\n",
    "    discriminator_model_fake, discriminator_logits_fake = discriminator(generator_model, reuse=True)\n",
    "    \n",
    "    smooth = 0.1\n",
    "    \n",
    "    discriminator_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_logits_real, labels=tf.ones_like(discriminator_model_real) * (1.0 - smooth)))\n",
    "    \n",
    "    discriminator_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_logits_fake, labels=tf.zeros_like(discriminator_model_fake)))\n",
    "    \n",
    "    generator_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator_logits_fake, labels=tf.ones_like(discriminator_model_fake)))\n",
    "    \n",
    "    discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
    "    \n",
    "    return discriminator_loss, generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Helper method to show output during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def show_generator_output(sess, n_images, input_z, out_channel_dim, image_mode):\n",
    "    \n",
    "    cmap = None if image_mode == 'RGB' else 'gray'\n",
    "    z_dim = input_z.get_shape().as_list()[-1]\n",
    "    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])\n",
    "\n",
    "    samples = sess.run(\n",
    "        generator(input_z, out_channel_dim, False),\n",
    "        feed_dict={input_z: example_z})\n",
    "\n",
    "    images_grid = helper.images_square_grid(samples, image_mode)\n",
    "    pyplot.imshow(images_grid, cmap=cmap)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(epoch_count, batch_size, z_dim, learning_rate, beta1, get_batches, data_shape, data_image_mode):\n",
    "    \n",
    "    input_real, input_z, lr = model_inputs(data_shape[1], data_shape[2], data_shape[3], z_dim)\n",
    "    discriminator_loss, generator_loss = model_loss(input_real, input_z, data_shape[3])\n",
    "    d_opt, g_opt = model_opt(discriminator_loss, generator_loss, lr, beta1)\n",
    "\n",
    "    steps = 0\n",
    "    print_every = 10\n",
    "    show_every = 100\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            for batch_images in get_batches(batch_size):\n",
    "\n",
    "                steps += 1\n",
    "                batch_images = batch_images*2\n",
    "                \n",
    "                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "\n",
    "                _ = sess.run(d_opt, feed_dict={input_real: batch_images, input_z: batch_z, lr: learning_rate})\n",
    "                _ = sess.run(g_opt, feed_dict={input_real: batch_images, input_z: batch_z, lr: learning_rate})\n",
    "            \n",
    "                if steps % print_every == 0:\n",
    "                    train_loss_d = discriminator_loss.eval(feed_dict={input_z: batch_z, input_real: batch_images})\n",
    "                    train_loss_g = generator_loss.eval(feed_dict={input_z: batch_z})\n",
    "                    print(\"Epoch {}..\".format(epoch_i+1),\n",
    "                          \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                          \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "\n",
    "                if steps % show_every == 0:\n",
    "                    show_generator_output(sess, batch_size, input_z, data_shape[3], data_image_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, labels, batch_size):\n",
    "    rand_choice = np.random.choice(data.shape[0], batch_size, replace=False)\n",
    "    \n",
    "    batch_x = data[rand_choice]\n",
    "    batch_y = labels[rand_choice]\n",
    "    \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "z_dim = 100\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.5\n",
    "epochs = 1\n",
    "\n",
    "# data_dir = './data'\n",
    "# import helper\n",
    "\n",
    "# celeba_dataset = helper.Dataset('celeba', glob(os.path.join(data_dir, 'img_align_celeba/*.jpg')))\n",
    "\n",
    "# with tf.Graph().as_default():\n",
    "#     train(epochs, batch_size, z_dim, learning_rate, beta1, celeba_dataset.get_batches,\n",
    "#           celeba_dataset.shape, celeba_dataset.image_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import image_loader as il\n",
    "image_loader = imp.reload(il)\n",
    "# MTG Card Loader testing\n",
    "# Load all image directory names\n",
    "images_path = \"../data/images/sets\"\n",
    "# sets = os.listdir(images_path)\n",
    "\n",
    "# Load preconfigured set_dimensions file\n",
    "sets = pd.read_csv(\"../data/images/set_dimensions_big.csv\",header=0)\n",
    "elves_set = image_loader.Imageset(sets, 'elf', images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# elves_images = load_images_type_dim(sets, 'elf', images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elves_images = elves_set.get_images()\n",
    "len(elves_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_a = celeba_dataset.get_batches(16)\n",
    "\n",
    "for image in batch_a:\n",
    "    print(image.shape)\n",
    "    print('a')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### GAN Resources\n",
    "\n",
    "Before I show some applications of GANs, here are some resources to get started with GANS:\n",
    "\n",
    "* The original [paper](https://arxiv.org/pdf/1406.2661.pdf) written by Ian Goodfellow in 2014. \n",
    "* Siraj Raval's [video tutorial](https://www.youtube.com/watch?v=-E2N1kQc8MM) on GANs (Really fun video)\n",
    "* This other [video](https://www.youtube.com/watch?v=deyOX6Mt_As) on GANS by Siraj Raval (Another fun video)\n",
    "* Ian Godfellow's [keynote](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks)\n",
    "\n",
    "### Applications of GANS\n",
    "\n",
    "Cool Applications of GANS:\n",
    "* [Generative Adversarial Text to Image Synthesis](https://github.com/paarthneekhara/text-to-image)\n",
    "* [Image super-resolution through deep learning](https://github.com/david-gpu/srez)\n",
    "* [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN)](https://github.com/junyanz/CycleGAN)\n",
    "* [Image-to-image translation with conditional adversarial nets (pix2pix)](https://github.com/phillipi/pix2pix)\n",
    "* [Deep Convolutional Generative Adversarial Networks (DCGAN)](https://github.com/Newmu/dcgan_code)\n",
    "* [Τensorflow implementation of Deep Convolutional Generative Adversarial Networks (DCGAN)](https://github.com/carpedm20/DCGAN-tensorflow)\n",
    "* [Generative Visual Manipulation on the Natural Image Manifold (iGAN)](https://github.com/junyanz/iGAN)\n",
    "* [Neural Photo Editing with Introspective Adversarial Networks](https://github.com/ajbrock/Neural-Photo-Editor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
